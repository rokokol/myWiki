---
title: 6. Maximum Likelihood Estimation (cont.) and the Method of Moments
media_link: https://www.youtube.com/watch?v=JTbZP0yt9qc
published: 2017-08-17
created: 2025-04-18
description: "MIT 18.650 Statistics for Applications, Fall 2016View the complete course: http://ocw.mit.edu/18-650F16Instructor: Philippe RigolletIn this lecture, Prof. Rigollet continued on maximum likelihood"
tags:
  - матстат
  - конспекты
  - учеба
  - mit
master: "[[Матстат Hub (лекции)]]"
icon: LiMaximize2
---

# Еще пара слов о методе максимального правдоподобия и метод моментов

[00:28](https://www.youtube.com/watch?t=28&v=JTbZP0yt9qc)
## Функция информации Фишера
> [!note] Информация Фишера
> $I(\theta)=\mathbf{Var}[l'(\theta)]=-\mathbf{E}[l''(\theta)]$, где $l(\theta)=L_{1}(x, \theta)$, $\theta \in \mathbb{R}$. Конечно, $L_{n}(x, \theta)$ — функция правдоподобия
> Докажем второе равенство: пусть $X$ обладает плотностью $f_{\theta}$. Тогда
> $\int\limits_{-\infty}^{\infty} f_{\theta}(x) \, dx=1\implies \frac{ \partial  }{ \partial \theta }\int\limits_{-\infty}^{\infty} f_{\theta}(x) \, d=0\implies \int\limits_{-\infty}^{\infty} \frac{ \partial^{n}  }{ \partial \theta^{n} }f_{\theta}(x) \, dx=0$. В это же время $l''(\theta)=\frac{ \partial^{2} }{ \partial \theta^{2}}\log f_{\theta}(x)=\frac{ \partial  }{ \partial \theta }\left( \frac{\frac{ \partial  }{ \partial \theta }f_{\theta}(x)}{f_{\theta}(x)} \right)$. Применяя формулу производной частного и подставляя ее в математическое ожидание, приходим к искомому результату: один занулится из-за первых равенств, а второй является дисперсией [13:27](https://www.youtube.com/watch?t=807&v=JTbZP0yt9qc)

> [!tip] Общий случай
> $I(\theta)=\mathbf{E}[\nabla l(\theta)\nabla l(\theta)^{T}]-\mathbf{E}[\nabla l(\theta)\mathbf{E}[\nabla l(\theta)]^{T}=-\mathbf{E}[\nabla^{2}l(\theta)]$

### Для чего нам нужна информация Фишера?
[20:40](https://www.youtube.com/watch?t=1277&v=JTbZP0yt9qc)
Чем меньше вторая производная, тем более выгнута функция — поэтому мы берем математическое ожидание $I(\theta)=-\mathbf{E}[l''(\theta)]$ отрицательным. Другими словами, минимизация функции, умноженной на -1 равносильна поиску ее максимума. 

### Когда метод Фишера уместен?
[23:16](https://www.youtube.com/watch?t=1396&v=JTbZP0yt9qc)
- Каждому $\theta$ зависит свое распределение
- $supp(\mathbf{P_{\theta}})$ не зависит от $\theta$ — в лежащем в основе KL-отклонении используется логарифм, который не определен в нуле
- $\theta^{*}$ не лежит на границе $\Theta$ — это мешает работать с производными
- $I(\theta)$ инвертируема в окрестности $\theta^{*}$

Тогда $\hat{\theta}_{n}^{MLE}$ удовлетворяет:в соответствии с $\mathbf{P}_{\theta}$
 1. $\hat{\theta}_{n}^{MLE}\overset{ \mathbf{P} }{ \to }\theta^{*}$ в соответствии с $\mathbf{P}_{\theta}$
 2. $\sqrt{ n }(\hat{\theta}_{n}^{MLE}-\theta^{*})\overset{ d }{ \to }\mathcal{N}(0,I(\theta^{*})^{-1})$ в соответствии с $\mathbf{P}_{\theta}$
 > Чем выше кривизна, тем выше $I(\theta)$ и, соответственно, ниже $I(\theta)^{-1}$

> [!tip] Информация Фишера связанна с квадратическим риском: $R(\theta)=\mathbf{E}[(\hat{\theta}-\theta)^{2}]=\mathbf{Var}(\hat{\theta})+\mathbf{Bias}^{2}(\theta)$, $\mathbf{Bias}(\hat{\theta})=\mathbf{E}[\hat{\theta}]-\theta$
> Например, выполняется теорема Крамера-Рао: $\mathbf{Var}(\hat{\theta})\geq \frac{1}{nI(\theta)}$. Она несложно вытекает из второго следствия

> [!note] [31:25](https://www.youtube.com/watch?t=1891&v=JTbZP0yt9qc) — учился бы я в MIT — попробовал бы доказать это... но, это выглядит максимально зловеще как-то да и термер я пока что не доучил, блин

## Метод моментов
[32:36](https://www.youtube.com/watch?t=1950&v=JTbZP0yt9qc)

